import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from googleapiclient.discovery import build
from textblob import TextBlob
from prophet import Prophet
from statsmodels.tsa.statespace.sarimax import SARIMAX 
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import NMF
from sklearn.preprocessing import MinMaxScaler, StandardScaler # å¼•å…¥ StandardScaler
import nltk
import re
import matplotlib.pyplot as plt 
# å¼•å…¥ PyTorch æ ¸å¿ƒæ¨¡çµ„
import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader

# ä¸‹è¼‰å¿…è¦çš„ NLTK æ•¸æ“š
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

# --- é é¢è¨­å®š ---
st.set_page_config(page_title="YouTube æƒ…æ„Ÿåˆ†æèˆ‡ LSTM æµé‡é æ¸¬ç³»çµ±", layout="wide")

st.title("ğŸ“Š YouTube æƒ…æ„Ÿåˆ†æèˆ‡ LSTM æµé‡é æ¸¬ç³»çµ±")
# st.markdown("""
# æœ¬ç³»çµ±æ•´åˆ **NLP æƒ…æ„Ÿåˆ†æ**èˆ‡ **PyTorch LSTM** æ¨¡å‹ã€‚
# **é æ¸¬è¨­å®šï¼š** ä½¿ç”¨æ‰€æœ‰æ­·å²æ•¸æ“šé€²è¡Œè¨“ç·´ï¼Œä¸¦å›ºå®šé æ¸¬**æœªä¾† 30 å¤©**çš„è¶¨å‹¢ã€‚
# """)

# --- å´é‚Šæ¬„ï¼šè¨­å®š ---
st.sidebar.header("âš™ï¸ åƒæ•¸è¨­å®š")
api_key = st.sidebar.text_input("è¼¸å…¥ YouTube Data API Key", type="password")
video_input = st.sidebar.text_input("è¼¸å…¥ YouTube å½±ç‰‡ ID æˆ–ç¶²å€", value="fB8TyLTD7EE")
max_results = st.sidebar.slider("æŠ“å–è©•è«–æ•¸é‡ä¸Šé™ (æœ€å¤§ 50000 ç­†)", 100, 50000, 50000)
# å›ºå®šé æ¸¬æœŸç‚º 30 å¤©
FORECAST_PERIOD = 30 
LOOK_BACK = 7 # LSTM Lookback

# --- å‡½æ•¸å®šç¾©å€ ---

def extract_video_id(input_str):
    """å¾ç¶²å€æˆ–é«’äº‚çš„å­—ä¸²ä¸­æå–ç´”æ·¨çš„ Video ID"""
    if not input_str: return ""
    match_standard = re.search(r'v=([a-zA-Z0-9_-]{11})', input_str)
    if match_standard: return match_standard.group(1)
    match_short = re.search(r'youtu\.be/([a-zA-Z0-9_-]{11})', input_str)
    if match_short: return match_short.group(1)
    if '?' in input_str: return input_str.split('?')[0]
    return input_str.strip()

@st.cache_data(ttl=3600) 
def get_video_comments(api_key, video_id, max_results):
    """è³‡æ–™è’é›†ï¼šé€é YouTube API æŠ“å–è©•è«– (å…§å®¹èˆ‡åŸç‰ˆç›¸åŒ)"""
    if not api_key or not video_id: return pd.DataFrame()
    youtube = build('youtube', 'v3', developerKey=api_key)
    comments_data = []
    
    try:
        request = youtube.commentThreads().list(
            part="snippet", videoId=video_id, maxResults=100, textFormat="plainText"
        )
        
        while request and len(comments_data) < max_results:
            response = request.execute()
            for item in response['items']:
                comment = item['snippet']['topLevelComment']['snippet']
                comments_data.append({
                    'text': comment['textDisplay'],
                    'like_count': comment['likeCount'],
                    'published_at': comment['publishedAt'],
                    'author': comment['authorDisplayName']
                })
            
            if 'nextPageToken' in response and len(comments_data) < max_results:
                request = youtube.commentThreads().list(
                    part="snippet", videoId=video_id, maxResults=100,
                    textFormat="plainText", pageToken=response['nextPageToken']
                )
            else:
                break
    except Exception as e:
        st.error(f"API æŠ“å–éŒ¯èª¤: {e}")
        return pd.DataFrame()
        
    return pd.DataFrame(comments_data)

def analyze_sentiment(text):
    """æƒ…æ„Ÿåˆ†æ (Demo ä½¿ç”¨ TextBlob)"""
    return TextBlob(str(text)).sentiment.polarity

def extract_topics(texts, n_topics=5):
    """ä¸»é¡Œå»ºæ¨¡ (ä½¿ç”¨ NMF)"""
    if len(texts) < 5: return ["æ¨£æœ¬éå°‘ï¼Œç„¡æ³•æå–ä¸»é¡Œ"]
    tfidf = TfidfVectorizer(max_features=1000, stop_words='english')
    try:
        X = tfidf.fit_transform(texts)
        n_components = min(n_topics, X.shape[0], X.shape[1])
        if n_components < 2: return ["æ–‡æœ¬å…§å®¹ä¸è¶³ä»¥æå–é—œéµå­—"]
        nmf = NMF(n_components=n_components, random_state=42)
        nmf.fit(X)
        keywords = []
        feature_names = tfidf.get_feature_names_out()
        for topic_idx, topic in enumerate(nmf.components_):
            top_features_ind = topic.argsort()[:-6:-1]
            top_features = [feature_names[i] for i in top_features_ind]
            keywords.append(f"ä¸»é¡Œ {topic_idx+1}: {', '.join(top_features)}")
        return keywords
    except ValueError:
        return ["æ–‡æœ¬å…§å®¹ä¸è¶³ä»¥æå–é—œéµå­—"]

# --- PyTorch LSTM æ¨¡å‹å®šç¾© (å…§å®¹èˆ‡åŸç‰ˆç›¸åŒ) ---
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size=1):
        super(LSTMModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return out

def create_lstm_dataset(data, look_back=7, is_forecast=False):
    """å°‡æ™‚é–“åºåˆ—æ•¸æ“šè½‰æ›ç‚º LSTM æ¨¡å‹æ‰€éœ€çš„è¼¸å…¥æ ¼å¼ä¸¦é€²è¡Œç¸®æ”¾ (å…§å®¹èˆ‡åŸç‰ˆç›¸åŒ)"""
    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled_data = scaler.fit_transform(data['y'].values.reshape(-1, 1))
    
    if is_forecast:
        X = scaled_data[-look_back:, 0]
        return X, scaler

    X, Y = [], []
    for i in range(len(scaled_data) - look_back):
        a = scaled_data[i:(i + look_back), 0]
        X.append(a)
        Y.append(scaled_data[i + look_back, 0])
        
    return np.array(X), np.array(Y), scaler

# --- ç¹ªåœ–å‡½æ•¸ (å…§å®¹èˆ‡åŸç‰ˆç›¸åŒ) ---

def plot_lstm_results(train_dates, forecast_dates, actual_values, predictions, title):
    """ç¹ªè£½ LSTM é æ¸¬çµæœ (ä½¿ç”¨æ‰€æœ‰æ•¸æ“šè¨“ç·´ï¼Œé æ¸¬æœªä¾† N å¤©)"""
    fig = go.Figure()
    
    # è¨“ç·´æ•¸æ“š (æ‰€æœ‰æ­·å²å¯¦éš›æ•¸æ“š)
    fig.add_trace(go.Scatter(x=train_dates, y=actual_values, 
                             mode='lines+markers', name='æ­·å²æ•¸æ“š (ç”¨æ–¼è¨“ç·´)', 
                             line=dict(color='gray', width=1.5), marker=dict(size=4)))
    
    # é æ¸¬ç·š
    fig.add_trace(go.Scatter(x=forecast_dates, y=predictions, 
                             mode='lines+markers', name=f'LSTM é æ¸¬ (æœªä¾†{len(forecast_dates)}å¤©)', 
                             line=dict(color='purple', width=3), marker=dict(size=6)))
    
    fig.update_layout(title=title,
                      xaxis_title='æ—¥æœŸ', yaxis_title='æ¯æ—¥è©•è«–é‡',
                      hovermode='x unified')
    return fig

# --- ä¸»ç¨‹å¼é‚è¼¯ ---

if st.sidebar.button("é–‹å§‹åˆ†ææµç¨‹"):
    if not api_key:
        st.error("è«‹è¼¸å…¥ API Keyï¼")
    elif not video_input:
        st.error("è«‹è¼¸å…¥å½±ç‰‡ IDï¼")
    else:
        # è‡ªå‹•æ¸…æ´— Video ID
        clean_video_id = extract_video_id(video_input)
        st.sidebar.success(f"å·²è­˜åˆ¥å½±ç‰‡ ID: {clean_video_id}")

        # [ä¿®æ”¹] å»ºç«‹ Tabs
        tab1, tab2, tab3 = st.tabs(["1. è³‡æ–™è’é›† & å‰è™•ç†", "2. NLP æƒ…æ„Ÿèˆ‡ä¸»é¡Œ", f"3. PyTorch LSTM é æ¸¬ (æœªä¾†{FORECAST_PERIOD}å¤©)"])

        # --- éšæ®µ 1: è³‡æ–™è’é›† ---
        with tab1:
            st.subheader("ğŸ“¥ è³‡æ–™è’é›† (Data Collection)")
            st.markdown(f"ç›®æ¨™æŠ“å–ä¸Šé™ï¼š**{max_results} ç­†è©•è«–**ï¼Œä»¥ç²å–æœ€é•·æ­·å²æ•¸æ“šã€‚")
            with st.spinner(f"æ­£åœ¨å¾ YouTube æŠ“å–è³‡æ–™ (ID: {clean_video_id})..."):
                df = get_video_comments(api_key, clean_video_id, max_results)
            
            if not df.empty:
                # ç§»é™¤æ™‚å€è³‡è¨Š
                df['published_at'] = pd.to_datetime(df['published_at']).dt.tz_localize(None)
                st.success(f"æˆåŠŸæŠ“å– {len(df)} ç­†è©•è«–ï¼")
                
                # é¡¯ç¤ºæ•¸æ“šæ™‚é–“è·¨åº¦
                min_date = df['published_at'].min().strftime('%Y-%m-%d')
                max_date = df['published_at'].max().strftime('%Y-%m-%d')
                time_span = (df['published_at'].max().normalize() - df['published_at'].min().normalize()).days
                
                st.info(f"**æ•¸æ“šæ™‚é–“ç¯„åœï¼š** å¾ **{min_date}** åˆ° **{max_date}**ï¼Œå…±è¦†è“‹ç´„ **{time_span} å¤©** (ä¸é€£çºŒ)ã€‚")
                st.dataframe(df.head())
                
                # æ™‚é–“åˆ†ä½ˆåœ–
                fig_hist = px.histogram(df, x="published_at", title="è©•è«–ç™¼ä½ˆæ™‚é–“åˆ†ä½ˆ")
                st.plotly_chart(fig_hist, use_container_width=True)
            else:
                st.error("ç„¡æ³•ç²å–æ•¸æ“šï¼Œè«‹æª¢æŸ¥ API Key æˆ–å½±ç‰‡ ID æ˜¯å¦æ­£ç¢ºã€‚")
                st.stop()

        # --- éšæ®µ 2: NLP åˆ†æ ---
        with tab2:
            st.subheader("ğŸ§  æƒ…æ„Ÿåˆ†æèˆ‡ä¸»é¡Œå»ºæ¨¡ (NLP)")
            
            with st.spinner("æ­£åœ¨é€²è¡Œæƒ…æ„Ÿé‹ç®—..."):
                # æƒ…æ„Ÿåˆ†æ
                df['sentiment'] = df['text'].apply(analyze_sentiment)
                df['sentiment_label'] = df['sentiment'].apply(lambda x: 'æ­£é¢' if x > 0.05 else ('è² é¢' if x < -0.05 else 'ä¸­ç«‹'))
                
                col1, col2 = st.columns(2)
                with col1:
                    st.markdown("#### æƒ…æ„Ÿåˆ†ä½ˆ")
                    fig_pie = px.pie(df, names='sentiment_label', title='è©•è«–æƒ…æ„Ÿä½”æ¯”', color_discrete_sequence=px.colors.sequential.RdBu)
                    st.plotly_chart(fig_pie, use_container_width=True)
                
                with col2:
                    st.markdown("#### ä¸»é¡Œé—œéµå­— (Topic Modeling)")
                    topics = extract_topics(df['text'].dropna())
                    for t in topics:
                        st.write(f"- {t}")
                
                st.markdown("---")
                st.markdown("**æƒ…æ„Ÿéš¨æ™‚é–“è®ŠåŒ–è¶¨å‹¢**")
                # æŒ‰å¤©èšåˆ
                df['date'] = df['published_at'].dt.date
                daily_data = df.groupby('date').agg({
                    'sentiment': 'mean',
                    'text': 'count'
                }).reset_index().rename(columns={'text': 'volume'})
                daily_data['date'] = pd.to_datetime(daily_data['date'])
                
                # ç¹ªè£½æ¯æ—¥æƒ…æ„Ÿè¶¨å‹¢åœ–
                fig_trend = px.line(daily_data, x='date', y='sentiment', title='æ¯æ—¥å¹³å‡æƒ…æ„Ÿåˆ†æ•¸è¶¨å‹¢')
                st.plotly_chart(fig_trend, use_container_width=True)
                
                # =======================================================
                # [æ–°å¢] è©•è«–æ•¸é‡èˆ‡æƒ…æ„Ÿåˆ†æ•¸çš„é—œä¿‚åœ–è¡¨
                # =======================================================
                st.markdown("### ğŸ”— è©•è«–æ•¸é‡ (æµé‡) èˆ‡æƒ…æ„Ÿåˆ†æ•¸çš„é—œä¿‚")
                
                if len(daily_data) >= 20: # ç¢ºä¿è‡³å°‘æœ‰ 20 å¤©æ•¸æ“šå†ç¹ªè£½è¤‡é›œé—œä¿‚åœ–
                    
                    # 1. è¨ˆç®—ç›¸é—œä¿‚æ•¸
                    correlation = daily_data['sentiment'].corr(daily_data['volume'])
                    st.info(f"è§€çœ‹é‡æ›¿ä»£æŒ‡æ¨™ (è©•è«–æ•¸) èˆ‡æƒ…æ„Ÿåˆ†æ•¸çš„ç›¸é—œä¿‚æ•¸ r = {correlation:.3f}")
                    
                    # 2. ç¹ªè£½æ•£é»åœ– (ç”¨æ–¼åˆ¤æ–·ç·šæ€§é—œä¿‚)
                    fig_scatter = px.scatter(daily_data, x='sentiment', y='volume', 
                                             title=f'æƒ…æ„Ÿåˆ†æ•¸ vs. è©•è«–æ•¸é‡æ•£é»åœ– (r={correlation:.3f})',
                                             labels={'sentiment': 'å¹³å‡æƒ…æ„Ÿåˆ†æ•¸', 'volume': 'æ¯æ—¥è©•è«–æ•¸é‡'})
                    fig_scatter.update_layout(xaxis_range=[-1, 1]) # é™åˆ¶æƒ…æ„Ÿåˆ†æ•¸ç¯„åœåœ¨ -1 åˆ° 1
                    st.plotly_chart(fig_scatter, use_container_width=True)
                    
                    # 3. ç¹ªè£½é›™è»¸æ™‚é–“è¶¨å‹¢åœ– (ç”¨æ–¼åˆ¤æ–·æ»¯å¾Œæ€§)
                    
                    # æ¨™æº–åŒ–æ•¸æ“š (MinMaxScaler)
                    scaler_volume = MinMaxScaler()
                    scaler_sentiment = MinMaxScaler()
                    
                    daily_data['volume_norm'] = scaler_volume.fit_transform(daily_data[['volume']])
                    daily_data['sentiment_norm'] = scaler_sentiment.fit_transform(daily_data[['sentiment']])
                    
                    fig_dual_norm = go.Figure()
                    
                    # ç¹ªè£½æ¨™æº–åŒ–è©•è«–æ•¸é‡ (å·¦è»¸)
                    fig_dual_norm.add_trace(go.Scatter(x=daily_data['date'], y=daily_data['volume_norm'], 
                                                       name='è©•è«–æ•¸é‡ (æ¨™æº–åŒ–)', line=dict(color='blue', width=2)))
                    
                    # ç¹ªè£½æ¨™æº–åŒ–æƒ…æ„Ÿåˆ†æ•¸ (å³è»¸)
                    fig_dual_norm.add_trace(go.Scatter(x=daily_data['date'], y=daily_data['sentiment_norm'], 
                                                       name='æƒ…æ„Ÿåˆ†æ•¸ (æ¨™æº–åŒ–)', line=dict(color='red', width=2, dash='dot')))
                    
                    fig_dual_norm.update_layout(title='æ¨™æº–åŒ–è©•è«–æ•¸é‡èˆ‡æƒ…æ„Ÿåˆ†æ•¸è¶¨å‹¢å°æ¯” (åˆ¤æ–·æ»¯å¾Œæ€§)',
                                                xaxis_title='æ—¥æœŸ',
                                                yaxis_title='æ¨™æº–åŒ–æ•¸å€¼',
                                                hovermode='x unified')
                    st.plotly_chart(fig_dual_norm, use_container_width=True)
                
                else:
                    st.warning("æ•¸æ“šé»ä¸è¶³ 20 å¤©ï¼Œç„¡æ³•é€²è¡Œæœ‰æ•ˆçš„é—œä¿‚åˆ†æã€‚")

        # --- æ•¸æ“šæº–å‚™ (ä¾› LSTM ä½¿ç”¨) ---
        LOOK_BACK = 7 # LSTM Lookback
        # æª¢æŸ¥æ•¸æ“šé»æ˜¯å¦è¶³å¤ ï¼šLookback + é æ¸¬æœŸ
        if len(daily_data) < FORECAST_PERIOD + LOOK_BACK:
            st.warning(f"æ•¸æ“šé»éå°‘ (è‡³å°‘éœ€è¦ {FORECAST_PERIOD + LOOK_BACK} å¤©)ï¼Œç„¡æ³•é€²è¡Œ {FORECAST_PERIOD} å¤©çš„é æ¸¬ã€‚", icon="âš ï¸")
            prophet_df = None
        else:
            # ä½¿ç”¨æ‰€æœ‰æ­·å²æ•¸æ“šé€²è¡Œè¨“ç·´
            prophet_df = daily_data[['date', 'volume']].rename(columns={'date': 'ds', 'volume': 'y'}).reset_index(drop=True)
            
            st.sidebar.markdown("---")
            st.sidebar.info(f"**æ­·å²æ•¸æ“šå¤©æ•¸ï¼š** {len(prophet_df)} å¤©")
            st.sidebar.info(f"**é æ¸¬ç›®æ¨™ï¼š** æœªä¾† {FORECAST_PERIOD} å¤©")


        # --- éšæ®µ 3: PyTorch LSTM é æ¸¬ (å›ºå®šé æ¸¬æœªä¾† N å¤©) ---
        with tab3:
            st.subheader(f"ğŸ¤– PyTorch LSTM é æ¸¬ (è¨“ç·´æ‰€æœ‰æ­·å²æ•¸æ“šï¼Œé æ¸¬æœªä¾† {FORECAST_PERIOD} å¤©)")
            
            if prophet_df is not None:
                st.markdown("LSTM ç¶²è·¯å°‡å¾æ‰€æœ‰æ­·å²æ•¸æ“šä¸­å­¸ç¿’æ™‚é–“æ¨¡å¼ï¼Œä¸¦ä½¿ç”¨æ»¾å‹•é æ¸¬ (Rolling Forecast) ä¾†æ¨ç®—æœªä¾†è¶¨å‹¢ã€‚")
                
                INPUT_SIZE = 1       
                HIDDEN_SIZE = 50     
                NUM_LAYERS = 1       
                NUM_EPOCHS = 20      
                BATCH_SIZE = 1       
                
                device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
                st.sidebar.info(f"PyTorch ä½¿ç”¨è¨­å‚™: {device}")
                
                # 1. è¨“ç·´æ•¸æ“šæº–å‚™ (ä½¿ç”¨æ‰€æœ‰æ­·å²æ•¸æ“š)
                X_train_data, Y_train_data, scaler = create_lstm_dataset(prophet_df, look_back=LOOK_BACK, is_forecast=False)
                
                X_train = torch.tensor(X_train_data, dtype=torch.float32).unsqueeze(-1).to(device)
                Y_train = torch.tensor(Y_train_data, dtype=torch.float32).unsqueeze(-1).to(device)
                
                train_dataset = TensorDataset(X_train, Y_train)
                train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=False)
                
                st.info(f"LSTM è¨“ç·´æ•¸æ“šé»ï¼š{len(X_train)} ç­† (ç”¨æ–¼é æ¸¬åŸºç¤çš„æ­·å²æ•¸æ“šé»)")
                
                with st.spinner(f"æ­£åœ¨è¨“ç·´ PyTorch LSTM æ¨¡å‹ (Epochs={NUM_EPOCHS})..."):
                    
                    # 2. å»ºç«‹æ¨¡å‹ã€æå¤±å‡½æ•¸å’Œå„ªåŒ–å™¨
                    model = LSTMModel(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS).to(device)
                    criterion = nn.MSELoss()
                    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
                    
                    # 3. è¨“ç·´è¿´åœˆ
                    model.train()
                    for epoch in range(NUM_EPOCHS):
                        for i, (inputs, labels) in enumerate(train_loader):
                            outputs = model(inputs)
                            loss = criterion(outputs, labels)
                            
                            optimizer.zero_grad()
                            loss.backward()
                            optimizer.step()
                            
                    # 4. é€²è¡Œæœªä¾†é æ¸¬
                    model.eval() 
                    with torch.no_grad():
                        # 4a. åˆå§‹åŒ–é æ¸¬è¼¸å…¥ (æœ€å¾Œä¸€å€‹ look_back åºåˆ—)
                        last_sequence_np, _ = create_lstm_dataset(prophet_df, look_back=LOOK_BACK, is_forecast=True)
                        current_input = torch.tensor(last_sequence_np, dtype=torch.float32).unsqueeze(0).unsqueeze(-1).to(device) # Shape: (1, look_back, 1)
                        
                        forecast_predictions = []
                        
                        for _ in range(FORECAST_PERIOD):
                            # é æ¸¬ä¸‹ä¸€å€‹æ™‚é–“é»
                            predicted_value_tensor = model(current_input) # Shape: (1, 1)
                            forecast_predictions.append(predicted_value_tensor.cpu().numpy()[0, 0])
                            
                            # æ»¾å‹•é æ¸¬ï¼šæ›´æ–°è¼¸å…¥åºåˆ—
                            predicted_value_scaled = predicted_value_tensor.clone().detach() 
                            
                            # ç§»é™¤ç¬¬ä¸€å€‹å…ƒç´ ï¼Œä¸¦åœ¨æœ«å°¾æ·»åŠ æ–°é æ¸¬å€¼
                            new_input_np = current_input.cpu().numpy()[0, 1:, 0]
                            new_input_np = np.append(new_input_np, predicted_value_scaled.cpu().numpy()[0, 0])
                            
                            # æ›´æ–° current_input
                            current_input = torch.tensor(new_input_np, dtype=torch.float32).unsqueeze(0).unsqueeze(-1).to(device)
                        
                    # 5. åå‘ç¸®æ”¾æ•¸æ“šï¼Œé‚„åŸç‚ºåŸå§‹è©•è«–é‡
                    forecast_predictions = np.array(forecast_predictions).reshape(-1, 1)
                    
                    # é‡æ–°ç²å– scalerï¼Œç”¨æ–¼åå‘è½‰æ›
                    _, _, final_scaler = create_lstm_dataset(prophet_df, look_back=LOOK_BACK, is_forecast=False)
                    
                    final_predictions = final_scaler.inverse_transform(forecast_predictions).flatten()
                    
                    # 6. ç¹ªåœ–æº–å‚™ï¼šç”Ÿæˆæœªä¾†æ—¥æœŸ
                    last_date = prophet_df['ds'].max()
                    forecast_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=FORECAST_PERIOD)
                    
                    st.write(f"#### ğŸš€ æœªä¾† {FORECAST_PERIOD} å¤©é æ¸¬çµæœ")
                    forecast_table = pd.DataFrame({
                        'æ—¥æœŸ': forecast_dates,
                        'é æ¸¬è©•è«–é‡': final_predictions.round(1).clip(min=0) # è©•è«–é‡ä¸èƒ½ç‚ºè² æ•¸
                    })
                    st.dataframe(forecast_table)

                    # ç¹ªè£½çµæœ
                    fig_lstm = plot_lstm_results(
                        train_dates=prophet_df['ds'],
                        forecast_dates=forecast_dates,
                        actual_values=prophet_df['y'].values,
                        predictions=final_predictions,
                        title=f'PyTorch LSTM æ¨¡å‹é æ¸¬ (é æ¸¬æœªä¾† {FORECAST_PERIOD} å¤©)'
                    )
                    st.plotly_chart(fig_lstm, use_container_width=True)
                    
            else:
                 pass

else:

    st.info("ğŸ‘ˆ è«‹åœ¨å´é‚Šæ¬„è¼¸å…¥è³‡æ–™ä¸¦é»æ“ŠæŒ‰éˆ•ï¼Œé–‹å§‹å¤šæ¨¡å‹åˆ†æã€‚")
